{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "import re\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tqdm\n",
    "import time\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.optimizer import Optimizer\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "6abdaf45543837fd3ecdeef340fdf57e5fdbd654"
   },
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "batch_size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "78b36954f557138daf62f407fbdf4d3bb0da21fa"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "5c0958fdac2475ba8b6a831007b361b7d2ad16ef"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f43d78c1501b69e1f5cbe5d58cb7f3ee7f70187"
   },
   "source": [
    "**数据处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "438a146b5e7c4a99f90db3191d673a9a3a185358"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }    \n",
    "def clean_text(x):\n",
    "    for dic in [contraction_mapping, mispell_dict, punct_mapping]:\n",
    "        for word in dic.keys():\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x   \n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text1(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "5136865814c735080c214c4251b916008ed9de97"
   },
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "339760fc6c985ef8f4e45f697899921c930985a7"
   },
   "outputs": [],
   "source": [
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    \n",
    "    # lower\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "        \n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text1(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text1(x))\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    # Clean numbers\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "    \n",
    "    print(\"Train shape : \",train_df.shape)\n",
    "    print(\"Test shape : \",test_df.shape)\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(train_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)   \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "    print(\"finish\")\n",
    "    \n",
    "    return train_X,test_X, train_y,tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e64035007c96c6517d0469a2ebe2ded3481c3b9"
   },
   "source": [
    "**预训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "8c1fabc74fede0d7511613dd5cd8c10014dbec1c"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2234b009100153e65e861f64cc41015173c6ef8f"
   },
   "source": [
    "**数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class QuraData(data.Dataset):\n",
    "    def __init__(self,questions,labels,augument=False,training=True):\n",
    "        super(QuraData, self).__init__()\n",
    "        self.augument=augument\n",
    "        self.questions=questions\n",
    "        self.labels= labels\n",
    "        self.len_ = len(self.questions)\n",
    "        self.training=training\n",
    "    def shuffle(self,d):\n",
    "        return np.random.permutation(d.tolist())\n",
    "\n",
    "    def dropout(self,d,p=0.5):\n",
    "        len_ = len(d)\n",
    "        index = np.random.choice(len_,int(len_*p))\n",
    "        d[index]=0\n",
    "        return d     \n",
    "    def __getitem__(self,index):\n",
    "        question,label =  self.questions[index],self.labels[index,np.newaxis]\n",
    "        \n",
    "        if self.training and self.augument :\n",
    "            augument=random.random()\n",
    "            if augument>0.7:\n",
    "                question= self.dropout(question,p=0.3)\n",
    "            elif augument>0.4:\n",
    "                question= self.shuffle(question)\n",
    "            else:\n",
    "                question= question\n",
    "            \"\"\"\n",
    "            if augument>0.5:\n",
    "                question= self.dropout(question,p=0.3)\n",
    "            else:\n",
    "                question= self.shuffle(question)\n",
    "            \"\"\"\n",
    "        question=torch.from_numpy(question).long()\n",
    "        label=torch.LongTensor(label).long()\n",
    "        return question,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "697ec5500495ebda91deb4a25199ed93c1c94b34"
   },
   "outputs": [],
   "source": [
    "def kmax_pooling(x, dim, k):\n",
    "    index = x.topk(k, dim = dim)[1].sort(dim = dim)[0]\n",
    "    return x.gather(dim, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "3d10861d8f98dc49df943bbbaf08009db2ed2696"
   },
   "outputs": [],
   "source": [
    "class LSTMText(nn.Module): \n",
    "    def __init__(self,embedding_matrix):\n",
    "        super(LSTMText, self).__init__()\n",
    "        self.model_name = 'LSTMText'\n",
    "        self.encoder = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "        self.title_lstm = nn.LSTM(input_size = embed_size,\\\n",
    "                            hidden_size = 192,\n",
    "                            num_layers = 2,\n",
    "                            bias = True,\n",
    "                            batch_first = False,\n",
    "                            #dropout = 0.5,\n",
    "                            bidirectional = True\n",
    "                            )\n",
    "        # self.dropout = nn.Dropout()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1152,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(0.2),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.dropout=nn.Dropout(0.25)\n",
    "        # self.fc = nn.Linear(3 * (opt.title_dim+opt.content_dim), opt.num_classes)\n",
    " \n",
    "    def forward(self, question):\n",
    "        question = self.encoder(question)\n",
    "        #question.detach()\n",
    "        question=self.dropout(question)\n",
    "        question_out = self.title_lstm(question.permute(1,0,2))[0].permute(1,2,0) \n",
    "        question_conv_out = kmax_pooling((question_out),2,3)\n",
    "        reshaped = question_conv_out.view(question_conv_out.size(0), -1)\n",
    "        logits = self.fc((reshaped))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "132549e69116f9a03db5133e9d3a55c45908098b"
   },
   "outputs": [],
   "source": [
    "train_X,test_X,train_y,word_index=load_and_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "0119ed12058882d5c1779080af0d9eb0efba39f8"
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2018).split(train_X, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "620aec41b7a6a49b4981e6a53763f43069d38489"
   },
   "outputs": [],
   "source": [
    "glove_embeddings = load_glove(word_index)\n",
    "paragram_embeddings = load_para(word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "afd80434ef5e55b1bee5b79310924c19c3bba4ab"
   },
   "outputs": [],
   "source": [
    "x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "48a1786800107894197a43d446d670c88d291187"
   },
   "outputs": [],
   "source": [
    "def train(model,n_epochs):\n",
    "    avg_losses_f = []\n",
    "    avg_val_losses_f = []\n",
    "    train_preds = np.zeros((len(train_X)))\n",
    "    # matrix for the predictions on the test set\n",
    "    test_preds = np.zeros((len(test_X)))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits): \n",
    "        train_dataset= QuraData(train_X[train_idx.astype(int)],train_y[train_idx.astype(int)],augument=True,training=True)\n",
    "        valid_dataset= QuraData(train_X[valid_idx.astype(int)],train_y[valid_idx.astype(int)],augument=False,training=False)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "         # make sure everything in the model is running on the GPU\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "        valid_preds_fold= np.zeros((len(valid_idx)))\n",
    "        test_preds_fold= np.zeros((len(test_X)))\n",
    "        print(f'Fold {i + 1}')\n",
    "        if i<3:\n",
    "            epochs=n_epochs\n",
    "        else:\n",
    "            epochs=1\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            model.train(True)\n",
    "            avg_loss = 0 \n",
    "            for ii,(x_batch,y_batch) in enumerate(train_loader):\n",
    "                x_batch=Variable(x_batch).cuda()\n",
    "                y_batch=Variable(y_batch).cuda()\n",
    "                y_pred = model(x_batch)\n",
    "                #print(y_pred.shape)\n",
    "                loss = loss_fn(y_pred, y_batch.float())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_loader)\n",
    "            model.train(False)\n",
    "            model.eval()   \n",
    "            avg_val_loss = 0.\n",
    "            with torch.no_grad():\n",
    "                for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                    x_batch=Variable(x_batch).cuda()\n",
    "                    y_batch=Variable(y_batch).cuda()\n",
    "                    y_pred = model(x_batch)\n",
    "                    avg_val_loss += loss_fn(y_pred, y_batch.float()).item() / len(valid_loader)\n",
    "                    valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "                elapsed_time = time.time() - start_time\n",
    "                train_preds[valid_idx] = valid_preds_fold\n",
    "                avg_losses_f.append(avg_loss)\n",
    "                avg_val_losses_f.append(avg_val_loss)\n",
    "                print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(epoch + 1, n_epochs, avg_loss, avg_val_loss,elapsed_time))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x_batch,) in enumerate(test_loader):\n",
    "                x_batch=Variable(x_batch).cuda()\n",
    "                y_pred = model(x_batch)\n",
    "                test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            test_preds+=test_preds_fold/len(splits)\n",
    "    print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))\n",
    "    return train_preds,test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "b562e9128809341007424b6e89bc7e52e5419fb5"
   },
   "outputs": [],
   "source": [
    "model_lstm=LSTMText(embedding_matrix=embedding_matrix).cuda()\n",
    "train_preds,test_preds=train(model_lstm,n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "a462fb9bf3204bbdcdbf2ba74ac9f5001f4e28ba"
   },
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(train_y, (train_preds > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))  \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true,
    "_uuid": "081e847ef3002dfb6744b7d285e46d35b1dc2ea7"
   },
   "outputs": [],
   "source": [
    "pred_test_y = (test_preds > best_thresh).astype(int)\n",
    "test_df = pd.read_csv(\"../input/test.csv\", usecols=[\"qid\"])\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
